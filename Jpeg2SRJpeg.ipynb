{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMkPfEnU/kwuNPStGn4xQ89",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhinav9629/JPEGUP/blob/main/Jpeg2SRJpeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "jNuTgLpi_-FF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e449087-fbb9-4b20-caa4-53c93807476f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.models import vgg19\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from torchvision.utils import save_image\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "iI4x6mN2Iny7"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# source = '/content/drive/MyDrive/SR_Data/DIV2K_train_HR'\n",
        "# destination = '/content/test_images'\n",
        "\n",
        "# # gather all files\n",
        "# filess = []\n",
        "# allfiles = os.listdir(source)\n",
        "# for files in allfiles:\n",
        "#   if files.endswith(\"x4.png\") and len(filess) < 21:\n",
        "#     filess.append(files)\n",
        "\n",
        "# # iterate on all files to move them to destination folder\n",
        "# for f in filess:\n",
        "#     src_path = os.path.join(source, f)\n",
        "#     dst_path = os.path.join(destination, f)\n",
        "#     shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "3ZFvws0w5J-i"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "load_model = False\n",
        "save_model = True\n",
        "chkpt_gen = \"/content/drive/MyDrive/SR_Data/SR_Checkpoints/generator.pth.tar\"\n",
        "chkpt_disc = \"/content/drive/MyDrive/SR_Data/SR_Checkpoints/discriminator.pth.tar\"\n",
        "learning_rate = 1e-4\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "num_workers = 4\n",
        "high_res_size = 96\n",
        "low_res_size = 24\n",
        "img_channels = 3"
      ],
      "metadata": {
        "id": "-9HEWFzlRsZX"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_res_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean=[0.5 for _ in range(img_channels)], std=[0.5 for _ in range(img_channels)]),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "low_res_transform = A.Compose(\n",
        "    [\n",
        "        A.Resize(width = low_res_size, height = low_res_size, interpolation = Image.BICUBIC),\n",
        "        A.Normalize(\n",
        "            mean = [0 for _ in range(img_channels)], std = [1 for _ in range(img_channels)]\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "both_transform = A.Compose(\n",
        "    [\n",
        "          A.RandomCrop(width = high_res_size , height = high_res_size),\n",
        "          A.HorizontalFlip(p = 0.5),\n",
        "          A.RandomRotate90(p = 0.5),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(\n",
        "            mean = [0 for _ in range(img_channels)], std = [1 for _ in range(img_channels)]\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "B_inEfjsT5xX"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset():\n",
        "  def __init__(self, path_dir):\n",
        "    self.data = []\n",
        "    self.target = []\n",
        "    self.path_dir = path_dir\n",
        "\n",
        "    for files in os.listdir(self.path_dir):\n",
        "      if files.endswith(\"x4.png\"):\n",
        "        self.data.append(files)\n",
        "      else:\n",
        "        self.target.append(files)\n",
        "\n",
        "    self.data = sorted(self.data)\n",
        "    self.target = sorted(self.target)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label_name = self.data[index]\n",
        "    image_name = self.target[index]\n",
        "    image = np.asarray(Image.open(os.path.join(self.path_dir,image_name)))\n",
        "    image = both_transform(image=image)[\"image\"]\n",
        "    high_res = high_res_transform(image = image)[\"image\"]\n",
        "    low_res = low_res_transform(image = image)[\"image\"]\n",
        "\n",
        "    return low_res, high_res"
      ],
      "metadata": {
        "id": "Q159jHlrbU3P"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class C_Block(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch, kernel_size, stride, padding, disc = False, use_activation = True, use_batchnorm = True ):\n",
        "     super().__init__()\n",
        "     self.convnet = nn.Conv2d(in_ch, out_ch, kernel_size = kernel_size, stride = stride, padding = padding, bias = not use_batchnorm)\n",
        "     self.bn = nn.BatchNorm2d(out_ch) if use_batchnorm else nn.Identity()\n",
        "     self.act = nn.LeakyReLU(0.2, inplace= True) if disc else nn.PReLU(num_parameters = out_ch)\n",
        "     self.use_activation = use_activation\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.convnet(input)\n",
        "    x = self.bn(x)\n",
        "    return self.act(x) if self.use_activation else x"
      ],
      "metadata": {
        "id": "d29zNaRiJvY4"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class US_Block(nn.Module):\n",
        "  def __init__(self, in_ch, scale):\n",
        "    super().__init__()\n",
        "    self.convnet = nn.Conv2d(in_ch, in_ch*scale**2, kernel_size = 3, stride = 1, padding=1)\n",
        "    self.ps = nn.PixelShuffle(scale)\n",
        "    self.act = nn.PReLU(num_parameters = in_ch)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.convnet(input)\n",
        "    x = self.ps(x)\n",
        "    return self.act(x)"
      ],
      "metadata": {
        "id": "jinoT4VODdYy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualNet_Block(nn.Module):\n",
        "  def __init__(self, in_ch):\n",
        "    super().__init__()\n",
        "    self.block1 = C_Block(in_ch, in_ch, kernel_size = 3, stride = 1, padding = 1 )\n",
        "    self.block2 = C_Block(in_ch, in_ch, kernel_size = 3, stride = 1, padding = 1, use_activation = False)\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.block1(input)\n",
        "    out = self.block2(x)\n",
        "    return out + input"
      ],
      "metadata": {
        "id": "PJNGCAf2XCjd"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, in_ch = 3, num_ch = 64, num_blocks = 16):\n",
        "    super().__init__()\n",
        "    self.init_convnet = C_Block(in_ch, num_ch, kernel_size = 9, stride = 1, padding = 4, use_batchnorm=False)\n",
        "    self.residual = nn.Sequential(\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "        ResidualNet_Block(num_ch),\n",
        "    )\n",
        "    self.convnet = C_Block(num_ch, num_ch, kernel_size = 3, stride = 1, padding = 1, use_activation = False)\n",
        "    self.upsamples = nn.Sequential(\n",
        "        US_Block(num_ch,scale = 2),\n",
        "        US_Block(num_ch,scale = 2),\n",
        "    )\n",
        "    self.final_convnet = nn.Conv2d(num_ch, in_ch, kernel_size = 9, stride = 1, padding = 4)\n",
        "\n",
        "  def forward(self, input):\n",
        "    initial = self.init_convnet(input)\n",
        "    x = self.residual(initial)\n",
        "    x = self.convnet(x) + initial\n",
        "    x = self.upsamples(x)\n",
        "    out = self.final_convnet(x)\n",
        "    return torch.tanh(out)\n"
      ],
      "metadata": {
        "id": "VAjbca6ufaru"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, in_ch = 3, features = 64):\n",
        "    super().__init__()\n",
        "    self.convnet = nn.Sequential(\n",
        "        C_Block(in_ch, features, kernel_size = 3, stride = 1, padding = 1, disc = True, use_activation = True, use_batchnorm = False ),\n",
        "        C_Block(features, features, kernel_size = 3, stride = 2, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "        C_Block(features, features*2, kernel_size = 3, stride = 1, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "        C_Block(features*2, features*2, kernel_size = 3, stride = 2, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "        C_Block(features*2, features*4, kernel_size = 3, stride = 1, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "        C_Block(features*4, features*4, kernel_size = 3, stride = 2, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "        C_Block(features*4, features*8, kernel_size = 3, stride = 1, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "        C_Block(features*8, features*8, kernel_size = 3, stride = 2, padding = 1, disc = True, use_activation = True, use_batchnorm = True ),\n",
        "    )\n",
        "    self.dense = nn.Sequential(\n",
        "        nn.AdaptiveAvgPool2d((6,6)),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(512*6*6, 1024),\n",
        "        nn.LeakyReLU(0.2, inplace=True),\n",
        "        nn.Linear(1024,1),\n",
        "    )\n",
        "\n",
        "  def forward(self, input):\n",
        "    x = self.convnet(input)\n",
        "    out = self.dense(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "PgaTePX4vYB2"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test():\n",
        "#   low_res = 24\n",
        "#   with torch.cuda.amp.autocast():\n",
        "#     x = torch.randn((5, 3, low_res,low_res))\n",
        "#     gen = Generator()\n",
        "#     disc = Discriminator()\n",
        "#     gen_out = gen(x)\n",
        "#     disc_out = disc(gen_out)\n",
        "#     print(gen_out.shape)\n",
        "#     print(disc_out.shape)\n",
        "# test()"
      ],
      "metadata": {
        "id": "SRjvnp6d0oCo"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VGGLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.vgg = vgg19(pretrained = True).features[:36].eval().to(device)\n",
        "    self.loss = nn.MSELoss()\n",
        "\n",
        "    for param in self.vgg.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  def forward(self, input, target):\n",
        "    vgg_inp_feat  = self.vgg(input)\n",
        "    vgg_tar_feat  = self.vgg(target)\n",
        "    return self.loss(vgg_inp_feat, vgg_tar_feat)"
      ],
      "metadata": {
        "id": "ey2vvNDw1yHd"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_chkpt(model, optimizer, path=\"my_checkpoint.pth.tar\"):\n",
        "  print(\"=> Saving Checkpoint\")\n",
        "  checkpoint = {\n",
        "      \"state_dict\" : model.state_dict(),\n",
        "      \"optimizer\" : optimizer.state_dict()\n",
        "  }\n",
        "  torch.save(checkpoint,path)\n",
        "\n",
        "def load_chkpt(checkpoint_file, model, optimizer, lr):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr"
      ],
      "metadata": {
        "id": "bQB2cPUYRdYV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_examples(low_res_folder, gen):\n",
        "    files = os.listdir(low_res_folder)\n",
        "    gen.eval()\n",
        "    for fil in files:\n",
        "        image = Image.open(os.path.join(low_res_folder,fil))\n",
        "        with torch.no_grad():\n",
        "            upscaled_img = gen(\n",
        "                 test_transform(image=np.asarray(image))[\"image\"]\n",
        "                .unsqueeze(0)\n",
        "                .to(device)\n",
        "            )\n",
        "        save_image(upscaled_img * 0.5 + 0.5, f\"/content/saved/{fil}\")\n",
        "    gen.train()"
      ],
      "metadata": {
        "id": "k5SwN525SkU1"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path  = \"/content/drive/MyDrive/SR_Data/DIV2K_train_HR\"\n",
        "dataset = ImageDataset(path_dir = train_path)\n",
        "loader = DataLoader(dataset,batch_size = batch_size, shuffle = True, pin_memory = True, num_workers=2)"
      ],
      "metadata": {
        "id": "Lx2tdEDPTrbp"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = Generator(in_ch = 3).to(device)\n",
        "discriminator = Discriminator(in_ch = 3).to(device)\n",
        "opt_gen = optim.Adam(generator.parameters(), lr = learning_rate, betas = (0.9,0.999))\n",
        "opt_disc = optim.Adam(discriminator.parameters(), lr = learning_rate, betas = (0.9,0.999))\n",
        "mse_loss = nn.MSELoss()\n",
        "bce_loss = nn.BCEWithLogitsLoss()\n",
        "vgg_loss = VGGLoss()"
      ],
      "metadata": {
        "id": "6CWT6TAsUwmM"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if load_model:\n",
        "  load_chkpt(chkpt_gen, generator, opt_gen, learning_rate)\n",
        "  load_chkpt(chkpt_disc, discriminator, opt_disc, learning_rate)"
      ],
      "metadata": {
        "id": "bD2TRua7Smuq"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  loop = tqdm(loader, leave= True)\n",
        "  loop.set_description(f\"Epoch {epoch}:\")\n",
        "\n",
        "  for batch_idx, (low_res, high_res) in enumerate(loop):\n",
        "    low_res = low_res.to(device)\n",
        "    high_res = high_res.to(device)\n",
        "\n",
        "    fake = generator(low_res)\n",
        "    # disc_real = discriminator(high_res)\n",
        "    # disc_fake = discriminator(fake.detach())\n",
        "    # disc_loss_real = bce_loss(disc_real, torch.ones_like(disc_real) - 0.1 * torch.rand_like(disc_real))\n",
        "    # disc_loss_fake = bce_loss(disc_fake, torch.zeros_like(disc_fake))\n",
        "    # disc_loss  = disc_loss_real + disc_loss_fake\n",
        "\n",
        "    # opt_disc.zero_grad()\n",
        "    # disc_loss.backward()\n",
        "    # opt_disc.step()\n",
        "\n",
        "    l2_loss = mse_loss(fake,high_res)\n",
        "    #disc_fake = discriminator(fake)\n",
        "    # gen_gan_loss = 1e-3*bce_loss(disc_fake, torch.ones_like(disc_fake))\n",
        "    # vgg19_loss = 0.006*vgg_loss(fake,high_res)\n",
        "    #gen_loss  = gen_gan_loss + vgg19_loss\n",
        "    gen_loss = l2_loss\n",
        "\n",
        "    opt_gen.zero_grad()\n",
        "    gen_loss.backward()\n",
        "    opt_gen.step()\n",
        "\n",
        "    if batch_idx % 200 == 0:\n",
        "      plot_examples(\"/content/test_images\",generator)\n",
        "\n",
        "\n",
        "  if save_model and epoch%5 == 0:\n",
        "    print(f\"Epoch: {epoch}, LossG_L2: {gen_loss}\")\n",
        "    save_chkpt(generator, opt_gen, path = chkpt_gen)\n",
        "    save_chkpt(discriminator, opt_disc, path = chkpt_disc)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bev6fwsGh3i3",
        "outputId": "48b094c1-9fbb-4def-9c8f-1aad73385d21"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:: 100%|██████████| 50/50 [01:48<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, LossG_L2: 0.045795682817697525\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:: 100%|██████████| 50/50 [01:42<00:00,  2.05s/it]\n",
            "Epoch 2:: 100%|██████████| 50/50 [01:42<00:00,  2.06s/it]\n",
            "Epoch 3:: 100%|██████████| 50/50 [01:45<00:00,  2.11s/it]\n",
            "Epoch 4:: 100%|██████████| 50/50 [01:44<00:00,  2.10s/it]\n",
            "Epoch 5:: 100%|██████████| 50/50 [01:46<00:00,  2.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, LossG_L2: 0.027267929166555405\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6:: 100%|██████████| 50/50 [01:48<00:00,  2.18s/it]\n",
            "Epoch 7:: 100%|██████████| 50/50 [01:46<00:00,  2.12s/it]\n",
            "Epoch 8:: 100%|██████████| 50/50 [01:48<00:00,  2.18s/it]\n",
            "Epoch 9:: 100%|██████████| 50/50 [01:48<00:00,  2.18s/it]\n",
            "Epoch 10:: 100%|██████████| 50/50 [01:48<00:00,  2.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, LossG_L2: 0.04217223450541496\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11:: 100%|██████████| 50/50 [01:53<00:00,  2.28s/it]\n",
            "Epoch 12:: 100%|██████████| 50/50 [01:50<00:00,  2.22s/it]\n",
            "Epoch 13:: 100%|██████████| 50/50 [01:49<00:00,  2.20s/it]\n",
            "Epoch 14:: 100%|██████████| 50/50 [01:49<00:00,  2.19s/it]\n",
            "Epoch 15:: 100%|██████████| 50/50 [01:52<00:00,  2.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, LossG_L2: 0.018400048837065697\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 17:: 100%|██████████| 50/50 [01:53<00:00,  2.28s/it]\n",
            "Epoch 18:: 100%|██████████| 50/50 [01:52<00:00,  2.25s/it]\n",
            "Epoch 19:: 100%|██████████| 50/50 [01:52<00:00,  2.26s/it]\n",
            "Epoch 20:: 100%|██████████| 50/50 [01:52<00:00,  2.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, LossG_L2: 0.02584148943424225\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21:: 100%|██████████| 50/50 [01:52<00:00,  2.25s/it]\n",
            "Epoch 22:: 100%|██████████| 50/50 [01:52<00:00,  2.24s/it]\n",
            "Epoch 23:: 100%|██████████| 50/50 [01:52<00:00,  2.24s/it]\n",
            "Epoch 24:: 100%|██████████| 50/50 [01:54<00:00,  2.28s/it]\n",
            "Epoch 25:: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25, LossG_L2: 0.01779075711965561\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
            "Epoch 27:: 100%|██████████| 50/50 [01:55<00:00,  2.30s/it]\n",
            "Epoch 28:: 100%|██████████| 50/50 [01:54<00:00,  2.30s/it]\n",
            "Epoch 29:: 100%|██████████| 50/50 [01:54<00:00,  2.28s/it]\n",
            "Epoch 30:: 100%|██████████| 50/50 [01:55<00:00,  2.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30, LossG_L2: 0.015067875385284424\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 32:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
            "Epoch 33:: 100%|██████████| 50/50 [01:55<00:00,  2.30s/it]\n",
            "Epoch 34:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
            "Epoch 35:: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 35, LossG_L2: 0.02820739522576332\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36:: 100%|██████████| 50/50 [01:57<00:00,  2.34s/it]\n",
            "Epoch 37:: 100%|██████████| 50/50 [01:58<00:00,  2.36s/it]\n",
            "Epoch 38:: 100%|██████████| 50/50 [02:01<00:00,  2.43s/it]\n",
            "Epoch 39:: 100%|██████████| 50/50 [02:00<00:00,  2.41s/it]\n",
            "Epoch 40:: 100%|██████████| 50/50 [01:57<00:00,  2.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40, LossG_L2: 0.014905921183526516\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41:: 100%|██████████| 50/50 [01:56<00:00,  2.34s/it]\n",
            "Epoch 42:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
            "Epoch 43:: 100%|██████████| 50/50 [01:56<00:00,  2.34s/it]\n",
            "Epoch 44:: 100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n",
            "Epoch 45:: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 45, LossG_L2: 0.014099973253905773\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46:: 100%|██████████| 50/50 [01:57<00:00,  2.36s/it]\n",
            "Epoch 47:: 100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n",
            "Epoch 48:: 100%|██████████| 50/50 [01:56<00:00,  2.34s/it]\n",
            "Epoch 49:: 100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n",
            "Epoch 50:: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, LossG_L2: 0.027164433151483536\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51:: 100%|██████████| 50/50 [01:57<00:00,  2.34s/it]\n",
            "Epoch 52:: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it]\n",
            "Epoch 53:: 100%|██████████| 50/50 [01:56<00:00,  2.34s/it]\n",
            "Epoch 54:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
            "Epoch 55:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 55, LossG_L2: 0.014375108294188976\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 56:: 100%|██████████| 50/50 [01:57<00:00,  2.35s/it]\n",
            "Epoch 57:: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it]\n",
            "Epoch 58:: 100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
            "Epoch 59:: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it]\n",
            "Epoch 60:: 100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 60, LossG_L2: 0.011683267541229725\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 61:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 62:: 100%|██████████| 50/50 [01:54<00:00,  2.30s/it]\n",
            "Epoch 63:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 64:: 100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n",
            "Epoch 65:: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 65, LossG_L2: 0.007787991315126419\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 67:: 100%|██████████| 50/50 [01:53<00:00,  2.26s/it]\n",
            "Epoch 68:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 69:: 100%|██████████| 50/50 [01:54<00:00,  2.28s/it]\n",
            "Epoch 70:: 100%|██████████| 50/50 [01:54<00:00,  2.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 70, LossG_L2: 0.017173418775200844\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 71:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 72:: 100%|██████████| 50/50 [01:53<00:00,  2.26s/it]\n",
            "Epoch 73:: 100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
            "Epoch 74:: 100%|██████████| 50/50 [01:56<00:00,  2.33s/it]\n",
            "Epoch 75:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 75, LossG_L2: 0.02173822931945324\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 77:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 78:: 100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
            "Epoch 79:: 100%|██████████| 50/50 [01:53<00:00,  2.28s/it]\n",
            "Epoch 80:: 100%|██████████| 50/50 [01:54<00:00,  2.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 80, LossG_L2: 0.017914051190018654\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 82:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 83:: 100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
            "Epoch 84:: 100%|██████████| 50/50 [01:55<00:00,  2.31s/it]\n",
            "Epoch 85:: 100%|██████████| 50/50 [01:55<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 85, LossG_L2: 0.009608452208340168\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86:: 100%|██████████| 50/50 [01:54<00:00,  2.29s/it]\n",
            "Epoch 87:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 88:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 89:: 100%|██████████| 50/50 [01:54<00:00,  2.28s/it]\n",
            "Epoch 90:: 100%|██████████| 50/50 [01:52<00:00,  2.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 90, LossG_L2: 0.02515692636370659\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 91:: 100%|██████████| 50/50 [01:54<00:00,  2.30s/it]\n",
            "Epoch 92:: 100%|██████████| 50/50 [01:54<00:00,  2.28s/it]\n",
            "Epoch 93:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 94:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 95:: 100%|██████████| 50/50 [01:54<00:00,  2.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 95, LossG_L2: 0.02725205384194851\n",
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 96:: 100%|██████████| 50/50 [01:53<00:00,  2.28s/it]\n",
            "Epoch 97:: 100%|██████████| 50/50 [01:52<00:00,  2.26s/it]\n",
            "Epoch 98:: 100%|██████████| 50/50 [01:53<00:00,  2.27s/it]\n",
            "Epoch 99:: 100%|██████████| 50/50 [01:53<00:00,  2.26s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tOWYuYi-npjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_chkpt(generator, opt_gen, path = \"/content/drive/MyDrive/SR_Data/SR_Checkpoints/Epoch200/generator.pth.tar\")\n",
        "save_chkpt(discriminator, opt_disc, path = \"/content/drive/MyDrive/SR_Data/SR_Checkpoints/Epoch200/discriminator.pth.tar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xLAAqamnrjL",
        "outputId": "a5c38e63-722d-4339-f0de-94e39cbe26e6"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving Checkpoint\n",
            "=> Saving Checkpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyUDDWkdoBed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}